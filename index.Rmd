--- 
title: "Untitled" 
author: "Anonymous" 
output: html_document 
---


```{r setup, include=FALSE} 
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = FALSE
)
library(tidyverse)
library(skimr)

function_files <- list.files("./functions", full.names = TRUE)

for (file in function_files) {
    source(file)
}


```

# Business Task  
The primary objective of this analysis is to explore the relationship between Bellabeat users and their usage of Bellabeat products. We aim to define user groups based on their habits and create usage scenarios for each group. By addressing the following questions, we will gain insights to inform Bellabeat’s marketing strategy and product development:  
1.	Are there distinct user groups based on product usage?  
2.	How do these user groups differ in their usage of the product?  
3.	How can this analysis benefit these user groups by profiling their needs?  
4.	Are there patterns in product usage, and what are the usage habits of each group?  
5.	Can we identify patterns in the data that can be leveraged to maximize benefits for all users?  
The task is to analyze the usage data from Bellabeat’s smart devices and app to identify distinct user groups and their respective usage patterns. Understanding these groups and their behaviors will enable Bellabeat to tailor its marketing strategies to better meet user needs, enhance user satisfaction, and drive growth. This analysis involves segmenting users based on their habits, identifying usage scenarios, and leveraging data patterns to provide actionable recommendations for maximizing user benefits.


# Data Sources  

The data for this analysis was sourced from Kaggle and is available at Kaggle - Fitbit Dataset. It consists of a zip file containing two datasets:  
1.	Fitabase Data 3.12.16-4.11.16  
2.	Fitabase Data 4.12.16-5.12.16  
For this exercise, we will use the Fitabase Data 4.12.16-5.12.16 dataset.  

## Data Files  
The dataset contains the following 18 CSV files:  
•	dailyActivity_merged.csv  
•	dailyCalories_merged.csv  
•	dailyIntensities_merged.csv  
•	dailySteps_merged.csv  
•	heartrate_seconds_merged.csv  
•	hourlyCalories_merged.csv  
•	hourlyIntensities_merged.csv  
•	hourlySteps_merged.csv  
•	minuteCaloriesNarrow_merged.csv  
•	minuteCaloriesWide_merged.csv  
•	minuteIntensitiesNarrow_merged.csv  
•	minuteIntensitiesWide_merged.csv  
•	minuteMETsNarrow_merged.csv  
•	minuteSleep_merged.csv  
•	minuteStepsNarrow_merged.csv  
•	minuteStepsWide_merged.csv  
•	sleepDay_merged.csv  
•	weightLogInfo_merged.csv  

## Data Loading  
The data has been unzipped and placed in the ./data folder. Each of these CSV files was loaded into the global environment as data frames using a custom load_csv_files function.

```{r, Load Dataframes, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
dataframes <- load_csv_files("./data")
```

## Data Overview  
Using the skim function, a preliminary overview of the data was obtained. The dataset consists of 18 CSV files that can be categorized as follows:

```{r, Skim, eval=FALSE, include=FALSE}
for (df_name in dataframes) {
  cat("Dataframe:", df_name, "\n")
  print(skim(get(df_name)))
  cat("\n")
}

```


•	Daily Data:  
o	dailyActivity_merged  
o	dailyCalories_merged  
o	dailyIntensities_merged  
o	dailySteps_merged  
•	Hourly Data:  
o	hourlyCalories_merged  
o	hourlyIntensities_merged  
o	hourlySteps_merged  
•	Minutely Data:  
o	minuteCaloriesNarrow_merged  
o	minuteCaloriesWide_merged  
o	minuteIntensitiesNarrow_merged  
o	minuteIntensitiesWide_merged  
o	minuteMETsNarrow_merged  
o	minuteSleep_merged  
o	minuteStepsNarrow_merged  
o	minuteStepsWide_merged  
•	Other Data:  
o	heartrate_seconds_merged  
o	sleepDay_merged  
o	weightLogInfo_merged   

## Data Merging  
•	Daily Data:  
o	dailyActivity_merged is already a merge of dailyCalories_merged, dailyIntensities_merged, and dailySteps_merged.  
o	sleepDay_merged can be joined with dailyActivity_merged as both contain daily records.  
•	Hourly Data:  
o	Hourly data files can be merged to create hourlyActivity_merged.  
•	Minutely Data:  
o	Minutely data files can be merged to create minuteActivity_merged.  
The merged datasets will provide a comprehensive view of user activity at different time intervals, enabling a detailed analysis of user habits and usage patterns.  

## Data Cleaning and Manipulation  
Before proceeding with the analysis, the following steps will be taken to clean and manipulate the data:  
1.	Handling Missing Values: Missing values will be identified and appropriately handled (e.g., imputation, removal).  
2.	Removing Duplicates: Duplicate records will be removed to ensure data integrity.  
3.	Data Type Conversion: Ensure all columns have the correct data types (e.g., datetime for date columns).  
4.	Merging Data: Merge the datasets as described to create comprehensive daily, hourly, and minutely datasets.  

## Data Preparation Steps  
1.	Drop Unnecessary DataFrames: Since dailyActivity_merged is already a combination of dailyCalories_merged, dailyIntensities_merged, and dailySteps_merged, these separate dataframes are not needed and were dropped from the environment.   

```{r, Remove Daily, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

rm(dailyCalories_merged, dailyIntensities_merged, dailySteps_merged)
names_to_remove <- c("dailyCalories_merged", "dailyIntensities_merged", "dailySteps_merged")
dataframes <- setdiff(dataframes, names_to_remove)
```


```{r, Remove Wide, include=FALSE}
names_to_remove <- dataframes[grep("Wide", dataframes)]
names_to_remove <- c(names_to_remove, "minuteSleep_merged")
dataframes <- setdiff(dataframes, names_to_remove)
rm(list = grep("Wide", ls(), value = TRUE), envir = .GlobalEnv)
```
2. Check for Missing Values:  
```{r, NA Check, echo=TRUE, message=TRUE, warning=TRUE}
for (df_name in dataframes) {
  cat( df_name, "dataframe", "has", sapply(df_name, function(x) sum(is.na(x))), "NAs.", "\n")
}

```
All data frames were checked for missing values. No missing values were found in any of the data frames.   
3. Remove Duplicates:  
```{r, Duplicate Check, echo=TRUE, message=TRUE, warning=TRUE}
for (df_name in dataframes) {
  df <- get(df_name)  
  num_duplicates <- sum(duplicated(df)) 
  cat(df_name, "has", num_duplicates, "duplicates.", "\n") 
}
```

•	minuteSleep_merged had 543 duplicates.
•	sleepDay_merged had 3 duplicates.

Duplicates were removed and the data was rechecked to confirm there were no remaining duplicates.
```{r, Remove Duplicate, echo=TRUE, message=TRUE, warning=TRUE}
dataframes_with_duplicates <- c("minuteSleep_merged", "sleepDay_merged")
minuteSleep_merged <- minuteSleep_merged[!duplicated(minuteSleep_merged), ]
sleepDay_merged <- sleepDay_merged[!duplicated(sleepDay_merged), ]


for (df_name in dataframes_with_duplicates) {
  df <- get(df_name)  
  num_duplicates <- sum(duplicated(df)) 
  cat(df_name, "has", num_duplicates, "duplicates.", "\n") 
}
```
4. Data Shape and Types: 

Each data frame was inspected for the number of observations and variables. The observations range from 940 to approximately 2.5 million. It was noted that the variables related to dates and times are stored as text data (<chr> data type), which can cause issues in comparisons and calculations.


a. Convert Date and Time Columns to Proper Data Types:   
Date and time columns were converted to appropriate datetime formats (POSIXct). This ensures proper handling and manipulation of date and time data.

```{r, Daily Posixct, include=FALSE}
dailyActivity_merged$Date <- as.POSIXct(dailyActivity_merged$ActivityDate, 
                                                format = "%m/%d/%Y", 
                                                tz = "UTC")

dailyActivity_merged <- subset(dailyActivity_merged, select = -ActivityDate)

```

b. Splitting Date and Time Columns:  
The datetime values need to be split and properly formatted in the other data frames as well. Given the number of data frames (14), this could be a time-consuming process. To streamline this, a split_datetime_column function was created to find and format these values efficiently. This function will be used iteratively on the data frames that are not dailyActivity_merged, splitting dates and times and then splitting the time values into Hour, Minute, and Second variables.


```{r, Split Datetime, include=FALSE}

filtered_dataframes <- dataframes[!grepl("daily", dataframes)]
for (df_name in filtered_dataframes) {
  cat(df_name, "... ")
  split_datetime_column(df_name)
  cat("done.", "\n")
}

```
### Handling Specific Data Frames
1. heartrate_seconds_merged  
The heartrate_seconds_merged dataframe has a seconds variable, which isn't directly comparable with other data frames. To make it useful, the mean heart rates for each minute will be calculated.


```{r, Heartrate, message=FALSE, warning=FALSE, include=FALSE}
heartrate_seconds_merged <- heartrate_seconds_merged %>% group_by(Id, Date, Hour, Minute) %>% summarise(mean_HR = mean(Value)) %>% group_by(Id, Date, Hour) %>% summarise(mean_HR = mean(mean_HR))
```
2. weightLogInfo_merged  
In the weightLogInfo_merged dataframe, only the LogId variable differentiates the information, and there is no way to relate it with time. Therefore, the data will be grouped by Id to obtain a mean weight for each person.


```{r, Weight, include=FALSE}
weightLogInfo_merged <- weightLogInfo_merged %>%
  select(Id, WeightKg, BMI) %>% group_by(Id) %>% summarise(mean_weight = mean(WeightKg), mean_BMI = mean(BMI))
```
### Creating Merged DataFrames  
1. Hourly Data  
The hourly_merged dataframe will be created by joining all the hourly* dataframes.

```{r, Hourly merged, include=FALSE}
names_to_remove <- dataframes[grep("hourly", dataframes)]
lapply(names_to_remove, drop_second_column)
dataframes <- setdiff(dataframes, names_to_remove)
hourly_merged <- full_join(hourlyCalories_merged, hourlyIntensities_merged)
hourly_merged <- full_join(hourly_merged, hourlySteps_merged)
dataframes <- c(dataframes, "hourly_merged")
rm(list = grep("^hourly(?!_)", ls(), perl = TRUE, value = TRUE), envir = .GlobalEnv)
hourly_merged <- hourly_merged %>% select(-Minute)
```

2. Minutely Data  
Similarly, the minute_merged dataframe will be created by joining all the minute* dataframes.


```{r, Minutely merged, include=FALSE}
names_to_remove <- dataframes[grep("minute", dataframes)]
lapply(names_to_remove, drop_second_column)
dataframes <- setdiff(dataframes, names_to_remove)
minutely_merged <- inner_join(minuteCaloriesNarrow_merged, minuteIntensitiesNarrow_merged)
minutely_merged <- inner_join(minutely_merged, minuteMETsNarrow_merged)
minutely_merged <- inner_join(minutely_merged, minuteStepsNarrow_merged)

dataframes <- c(dataframes, "minutely_merged")
rm(list = ls()[grepl("^minute(?!ly)", ls(), perl = TRUE)], envir = .GlobalEnv)

```
### Removing Unnecessary Columns  
After splitting the datetime columns, it was found that the Hour, Minute, and Second variables in the sleepDay_merged dataframe all contained 0, 0, 0 data. Therefore, these columns were removed to clean up the dataframe.

```{r, Sleep Merged, include=FALSE}
sleepDay_merged <- sleepDay_merged %>% select(-Hour, -Minute, -Second)
```
## Analyze

I had 5 questions to the data, which were  

1. Are there user groups which can be said to differ in the usage of the product?
2. In what respect those user groups differ?
3. How can this user grouping benefit this analysis, therefore profiling the needs of various user groups?
4. Are there any patterns in the usage of the product? What are the usage habits of user groups?
5. Are there any patterns in the data which can be leveraged to maximize the benefit for all users?


The variables I can use to group users can be;

* Time variables, like the time of the day, or the day of the week  
* Activity per day  
* Distance per day  
* Steps per day  
* Calories per day  
* Mean Heart Rate  
* Intensity per day  
* Sleep duration per day  
* Weight  
To address these questions, we will perform feature engineering and create new variables that can be used to group users. These variables include time-related variables, activity levels, distance, steps, calories, heart rate, intensity, sleep duration, and weight.
To analyze user behavior based on the day of the week, I created a weekdays data frame that maps dates to weekday names. This data frame was then joined with all other data frames containing date information to add the weekday information to each record. I also created a function to group hours of the day into categorical times: Morning, Afternoon, Evening, and Night. This function was applied to all data frames that contain hour data.

```{r, Weekdays Joins, include=FALSE}
rm(weekdays)
weekdays <- read_csv("./data/weekdays.csv", col_names = c("Date", "Day"), show_col_types = FALSE)

weekdays$Date <- as.POSIXct(weekdays$Date, 
                            format = "%d/%m/%Y",
                            tz = "UTC")

daily_activity <- inner_join(dailyActivity_merged, weekdays)
heartrate <- inner_join(heartrate_seconds_merged, weekdays)
hourly <- inner_join(hourly_merged, weekdays)
sleepday <- inner_join(sleepDay_merged, weekdays)
weight <- weightLogInfo_merged
names_to_remove <- c("dailyActivity_merged", "df", "heartrate_seconds_merged", "hourly_merged", "minutely_merged", "sleepDay_merged", "weightLogInfo_merged")
rm(list = (names_to_remove))

loaded_dataframes <- ls(envir = .GlobalEnv)[sapply(ls(envir = .GlobalEnv), function(x) inherits(get(x, envir = .GlobalEnv), "data.frame"))]
for (df_name in loaded_dataframes) {
  if ("Hour" %in% colnames(get(df_name, envir = .GlobalEnv))) {
    updated_df <- time_of_day(get(df_name, envir = .GlobalEnv))
    assign(df_name, updated_df, envir = .GlobalEnv)
  }
}
rm(updated_df)
```

From the dailyActivity_merged dataframe, I created a TotalMinutes variable as the sum of the minutes of different types of activities (e.g., SedentaryMinutes, LightlyActiveMinutes, FairlyActiveMinutes, and VeryActiveMinutes). Additionally, I created per-day variables for all metrics by grouping the data by user and date, storing this aggregated data in a new dataframe named per_day.

```{r message=FALSE, warning=FALSE, engineer features, message=FALSE, include=FALSE}

daily_activity$TotalMinutes <- daily_activity$VeryActiveMinutes + 
  daily_activity$FairlyActiveMinutes + 
  daily_activity$LightlyActiveMinutes + 
  daily_activity$SedentaryMinutes
per_day <-  daily_activity %>% select(Id, TotalSteps, TotalDistance, VeryActiveDistance, ModeratelyActiveDistance, LightActiveDistance, SedentaryActiveDistance, VeryActiveMinutes, FairlyActiveMinutes, LightlyActiveMinutes, SedentaryMinutes, Calories, TotalMinutes) %>% 
  group_by(Id) %>% 
  summarise(steps_per_day = mean(TotalSteps), 
            distance_per_day = mean(TotalDistance), 
            minutes_per_day = mean(TotalMinutes), 
            Calories_per_day = mean(Calories),
            VeryActiveDistance_per_day = mean(VeryActiveDistance), 
            ModeratelyActiveDistance_per_day = mean(ModeratelyActiveDistance),
            LightActiveDistance_per_day = mean(LightActiveDistance),
            SedentaryActiveDistance_per_day = mean(SedentaryActiveDistance),
            VeryActiveMinutes_per_day = mean(VeryActiveMinutes),
            FairlyActiveMinutes_per_day = mean(FairlyActiveMinutes),
            LightlyActiveMinutes_per_day = mean(LightlyActiveMinutes),
            SedentaryMinutes_per_day = mean(SedentaryMinutes)
            )
heartrate_daily <- heartrate %>% select(Id, Date, Hour, mean_HR) %>% group_by(Id) %>% summarise(mean_HR_day = mean(mean_HR))
sleep_per_day <- sleepday %>% select(Id, TotalMinutesAsleep, TotalTimeInBed, Date) %>% 
  group_by(Id) %>% summarise(mean_sleep_minutes = mean(TotalMinutesAsleep), 
                             mean_time_in_bed_minutes = mean(TotalTimeInBed))

per_day <- left_join(per_day, sleep_per_day)
```

            

## Share

Once you have completed your analysis, create your data visualizations. The visualizations should clearly communicate your high-level insights and recommendations. Use the following Case Study Roadmap as a guide:

Case Study Roadmap - Share

Guiding questions

- Were you able to answer the business questions?
- What story does your data tell?
- How do your findings relate to your original question?
- Who is your audience? What is the best way to communicate with them?
- Can data visualization help you share your findings?
- Is your presentation accessible to your audience?

Key tasks

1. Determine the best way to share your findings.
2. Create effective data visualizations.
3. Present your findings.
4. Ensure your work is accessible.

Deliverable
Supporting visualizations and key findings

Follow these steps:

1. Take out a piece of paper and a pen and sketch some ideas for how you will visualize the data. 
2. Once you choose a visual form, open your tool of choice to create your visualization. Use a presentation software, such as PowerPoint or Google Slides; your spreadsheet program; Tableau; or R.
3. Create your data visualization, remembering that contrast should be used to draw your audience’s attention to the most important insights. Use artistic principles including size, color, and shape.
4. Ensure clear meaning through the proper use of common elements, such as headlines, subtitles, and labels. 
5. Refine your data visualization by applying deep attention to detail.

## Act

Now that you have finished creating your visualizations, act on your findings. Prepare the deliverables you have been asked to create, including the high-level recommendations based on your analysis. Use the following Case Study Roadmap as a guide:

Case Study Roadmap - Act

Guiding questions

- What is your final conclusion based on your analysis?
- How could your team and business apply your insights?
- What next steps would you or your stakeholders take based on your findings?
- Is there additional data you could use to expand on your findings?

Key tasks
- Create your portfolio.
- Add your case study.
- Practice presenting your case study to a friend or family member.

Deliverable

Your top high-level insights based on your analysis

Follow these steps:

1. If you do not have one already, create an online portfolio. (Use Build a Portfolio with Google Sites.)
2. Consider how you want to feature your case study in your porftolio.
3. Upload or link your case study findings to your portfolio.
4. Write a brief paragraph describing the case study, your process, and your discoveries.
5. Add the paragraph to introduce your case study in your portfolio.